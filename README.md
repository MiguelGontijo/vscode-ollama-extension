# MigsIA - Extens√£o VS Code para Ollama

Extens√£o para Visual Studio Code que integra modelos de IA locais (via Ollama) e remotos (Claude, OpenRouter, DeepSeek).

## Recursos

- ü§ñ Integra√ß√£o com Ollama para modelos locais
- üåê Suporte para modelos remotos:
  - Claude 3 (Anthropic)
  - Mixtral (via OpenRouter)
  - DeepSeek Coder
- üí¨ Interface de chat integrada
- üîÑ Streaming de respostas em tempo real
- ‚öôÔ∏è Configura√ß√µes flex√≠veis

## Pr√©-requisitos

- Visual Studio Code 1.75.0 ou superior
- [Ollama](https://ollama.ai/) instalado para modelos locais
- Chaves de API configuradas para modelos remotos

## Instala√ß√£o

1. Instale a extens√£o atrav√©s do VS Code Marketplace
2. Configure as chaves de API necess√°rias
3. Configure o Ollama se desejar usar modelos locais

## Configura√ß√£o

A extens√£o oferece v√°rias op√ß√µes de configura√ß√£o:

- `migsIA.models.connectionType`: Tipo de conex√£o (local/remote/both)
- `migsIA.models.remote.*.apiKey`: Chaves de API para servi√ßos remotos
- `migsIA.ollama.*`: Configura√ß√µes do Ollama (URL, timeout, etc)

## Uso

1. Abra a sidebar da extens√£o
2. Selecione um modelo
3. Digite sua pergunta
4. Receba respostas em tempo real

## Desenvolvimento

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/vscode-ollama-extension.git

# Instale as depend√™ncias
npm install

# Compile a extens√£o
npm run compile

# Execute em modo de desenvolvimento
F5 no VS Code
```

## Licen√ßa

MIT